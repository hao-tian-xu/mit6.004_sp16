# <u>L09: Designing an Instruction Set</u>

- Systematic way to implement high-level FSM as a datapath + control FSM 
  - Is this implementation an FSM itself? 
  - If so, can you draw the truth table? 

### A Simple Programmable Datapath

<img src="image.assets/Screen Shot 2021-09-10 at 17.49.41.png" alt="Screen Shot 2021-09-10 at 17.49.41" style="zoom:33%;" />

- Each cycle, the Datapath: 
  - Reads two operands (a, b) from 4 registers (R0-R3) 
  - Performs one operation of +, -, *, NAND on operands 
  - Optionally writes result to a register 
- Control FSM

## The von Neumann Model

<img src="image.assets/Screen Shot 2021-09-10 at 16.24.26.png" alt="Screen Shot 2021-09-10 at 16.24.26" style="zoom:33%;" />

- Central processing unit
  - Performs operations on values in registers & memory
- Main memory
  - Array of W words of N bits each 
- Input/output devices
  - to communicate with the outside world 

### stored program computer

- Express program as a sequence of coded instructions 
- Memory holds both data and instructions 
- CPU fetches, interprets, and executes successive instructions of the program 

### Anatomy

<img src="image.assets/Screen Shot 2021-09-10 at 16.26.27.png" alt="Screen Shot 2021-09-10 at 16.26.27" style="zoom:33%;" />



## Instructions

- In a von Neumann machine, instructions are executed sequentially

### Instruction Set Architecture (ISA)

- ISA: The contract between software and hardware 
- The ISA is a new layer of abstraction

## The Beta

### Beta Storage

- CPU State
  - PC: program counter
  - 32 General Registers
    - 32-bit words
    - r31 hardwired to 0
- Main Memory
  - up to 2^32^ bytes (4GB) organized as 2^30^ 4-byte words

### Beta Instructions

- 3 types
  - Arithmetic and Logical
    - Perform operations on general registers
  - Loads and Stores
    - Move data between general registers and main memory 
  - Branches
    - Conditionally change the program counter (PC)
- All instructions have a <u>fixed length</u>: 32 bits (4 bytes) 
  - Tradeoff
    - Simpler decoding logic, next PC is easy to compute 
    - Larger code size 

### Beta ALU Instructions

- Format: <img src="image.assets/Screen Shot 2021-09-10 at 16.11.58.png" alt="Screen Shot 2021-09-10 at 16.11.58" style="zoom:33%;" />

- Assembly Language: `ADD(ra,rb,rc)`

  - ```pseudocode
    Reg[rc] ← Reg[ra] + Reg[rb]
    ```

- Implementation 

  -   <img src="image.assets/Screen Shot 2021-09-10 at 16.22.53.png" alt="Screen Shot 2021-09-10 at 16.22.53" style="zoom:33%;" />

- Instructions

  - arithmetic: ADD, SUB, MUL, DIV 
  - compare: CMPEQ, CMPLT, CMPLE 
  - boolean: AND, OR, XOR, XNOR 
  - shift: SHL, SHR, SAR

- Beta ALU Instructions with Constant 

  - Why?
    - Tradeoff
      - When used, they save registers and instructions 
      - More opcodes $\to$ more complex control logic and datapath
  - Format: <img src="image.assets/Screen Shot 2021-09-10 at 16.13.00.png" alt="Screen Shot 2021-09-10 at 16.13.00" style="zoom:33%;" />
  - Symbolic version: `ADDC(r1,-3,r3)`
  - Instructions
    - arithmetic: ADDC, SUBC, MULC, DIVC 
    - compare: CMPEQC, CMPLTC, CMPLEC 
    - boolean: ANDC, ORC, XORC, XNORC 
    - shift: SHLC, SHRC, SARC 
  - Implementation
    -  ​	<img src="image.assets/Screen Shot 2021-09-10 at 16.29.40.png" alt="Screen Shot 2021-09-10 at 16.29.40" style="zoom:33%;" />

### Beta Load and Store Instructions

- Format: <img src="image.assets/Screen Shot 2021-09-10 at 16.34.46.png" alt="Screen Shot 2021-09-10 at 16.34.46" style="zoom:33%;" />

- `LD(ra,const,rc)`

  - ```pseudocode
    Reg[rc] ← Mem[Reg[ra] + sext(const)]
    ```

- `ST(rc,const,ra)`

  - ```pseudocode
    Mem[Reg[ra] + sext(const)] ← Reg[rc]
    ```

### Beta Branch Instructions

- Format: <img src="image.assets/Screen Shot 2021-09-10 at 17.23.02.png" alt="Screen Shot 2021-09-10 at 17.23.02" style="zoom:33%;" />

- `BEQ(ra,offset,rc)`

  - ```pseudocode
    NPC ← PC + 4
    Reg[rc] ← NPC
    if (Reg[ra] == 0) 
    	PC ← NPC + 4*offset
    else
    	PC ← NPC
    ```

### Beta JMP Instruction

- Format: <img src="image.assets/Screen Shot 2021-09-10 at 17.21.18.png" alt="Screen Shot 2021-09-10 at 17.21.18" style="zoom:33%;" />

- `JMP(Ra,Rc)`

  - ```pseudocode
    Reg[Rc] ← PC + 4
    PC ← Reg[Ra] 
    ```

- Useful for procedure call return...



## *Thinking*

- A super exciting lecture. Understand the structure of computer design through basic logic, then step by step design a Beta version (of ISA).

- Introduced the von Neumann model with main memory, CPU, and IO.

  1. By the end of the lecture, there is a complete CPU architecture (other than the read and write of main memory).

    - Datapath, provides the control for all compute/logic units, including register
    - Control FSM provides control, including register (or constant and main memory address) and selection of computational/logical units.

  2. Instruction Set Architecture (ISA) as an abstraction of hardware as a contract between hardware and software

    - Similar to the spec in software engineering as a contract that allows the implementation to be tuned without users needing to care about the specific implementation



- 超级有趣的一节课，幸好没有放弃。通过基本的逻辑理解计算机设计的结构，然后一步步根据需要设计一个Beta版本（的ISA）。

- 介绍了冯诺依曼模型，包含主存储，CPU和IO。

  1. 这节课结束已经有了一个（除主存储读取之外的）完整的CPU（Central Processing Unit）的架构：

    - Datapath即为所有的计算/逻辑单元，包括Register
    - Control FSM提供控制，包括Register（或者constant和主存储地址）和计算/逻辑单元的选择

  2. 指令集架构（ISA）作为硬件的抽象作为软硬件之间的契约

    - 类似软件工程中的spec作为契约，使得实现可以调整，而使用者不需要在意具体的实现



# <u>L10: Assembly Language & Compilers</u>

## Technique

### How Does It Get Assembled?

- Load predefined symbols into a symbol table 
- Read input line by line 
  - Add symbols to symbol table as they are defined
  - Expand macros, translating symbols to values first 

### Registers are Predefined Symbols

- r0 = 0, …, r31 = 31

### Macros

- BETA Instructions

  - `.macro ADD(RA,RB,RC)	betaop(0x20,RA,RB,RC)` 

- Pseudoinstructions

  - ```assembly
    //	Convenience	macros	so	we	don’t	have	to	use	R31	
    .macro	LD(CC,RC) 				LD(R31,CC,RC)	
    .macro	ST(RA,CC) 				ST(RA,CC,R31)	
    .macro	BEQ(RA,LABEL) 		BEQ(RA,LABEL,R31)	
    .macro	BNE(RA,LABEL) 		BNE(RA,LABEL,R31)	
    
    .macro	MOVE(RA,RC) 			ADD(RA,R31,RC) 		//	Reg[RC]	<-	Reg[RA]	
    .macro	CMOVE(C,RC) 			ADDC(R31,C,RC) 		//	Reg[RC]	<-	C	
    .macro	COM(RA,RC) 				XORC(RA,-1,RC) 		//	Reg[RC]	<-	~Reg[RA]	
    .macro	NEG(RB,RC) 				SUB(R31,RB,RC) 		//	Reg[RC]	<-	-Reg[RB]	
    .macro	NOP() 						ADD(R31,R31,R31) 	//	do	nothing	
    
    .macro	BR(LABEL) 				BEQ(R31,LABEL) 		//	always	branch	
    .macro	BR(LABEL,RC) 			BEQ(R31,LABEL,RC) //	always	branch	
    .macro	CALL(LABEL) 			BEQ(R31,LABEL,LP) //	call	subroutine	
    .macro	BF(RA,LABEL,RC) 	BEQ(RA,LABEL,RC) 	//	0	is	false	
    .macro	BF(RA,LABEL) 			BEQ(RA,LABEL)	
    .macro	BT(RA,LABEL,RC) 	BNE(RA,LABEL,RC) 	//	1	is	true	
    .macro	BT(RA,LABEL) 			BNE(RA,LABEL)	
    
    //	Multi-instruction	sequences	
    .macro	PUSH(RA) 					ADDC(SP,4,SP)		ST(RA,-4,SP)	
    .macro	POP(RA) 					LD(SP,-4,RA)		ADDC(SP,-4,SP)
    ```

### Raw Data

- LONG assembles a 32-bit value 

  - Variables

  - Constants > 16 bits

  - ```assembly
    N: 		LONG(12)	
    factN:LONG(0xdeadbeef)	
    			…	
    Start:	
          LD(N,	r1)
          CMOVE(1,	r0)
    loop: MUL(r0,	r1,	r0)
          SUBC(r1,	1,	r1)
          BT(r1,	loop)	
          ST(r0,	factN)	
    ```

### `.` symbol

- The “.” (period) symbol means the next byte address to be filled

  - Can read or write to it 
  - Useful to control data layout or leave empty space (e.g., for arrays) 

- ```assembly
  .	= 0x100 	 			//	Assemble	into	0x100	
  LONG(0xdeadbeef)	
  k	=	. 						//	Symbol	“k”	has	value	0x104	
  LONG(0x00dec0de)	
  .	=	.+16 	 				//	Skip	16	bytes	
  LONG(0xc0ffeeee)
  ```

## Interpretation vs. Compilation

- Interpretation
  -   ​	<img src="image.assets/Screen Shot 2021-09-10 at 20.47.36.png" alt="Screen Shot 2021-09-10 at 20.47.36" style="zoom:33%;" />
- Compilation
  -   ​	<img src="image.assets/Screen Shot 2021-09-10 at 20.48.05.png" alt="Screen Shot 2021-09-10 at 20.48.05" style="zoom: 33%;" />



|                           | Interpretation    | Compilation                           |
| ------------------------- | ----------------- | ------------------------------------- |
| How it treats input “x+2” | Computes x+2      | Generates a program that computes x+2 |
| When it happens           | During execution  | Before execution                      |
| What it complicates/slows | Program execution | Program development                   |
| Decisions made at         | Run time          | Compile time                          |



## Compilers

### A Simple Compilation Strategy

-  Programs are sequences of statements, so repeatedly call `compile_statement(statement)`: 
   -  Unconditional: `expr;`	
   -  Compound: `{ statement1;	statement2;	... }`
   -  Conditional: `if (expr)	statement1;	else	statement2;`
   -  Iteration: `while (expr)	statement;`
-  Also need `compile_expr(expr)` to generate code to compute value of expr into a register 
   - Constants: `1234`
   - Variables: `a, b[expr]`
   - Assignment: `a = expr`
   - Operations: `expr1 +	expr2`, …
   - Procedure calls: `proc(expr,...)`

### Compiling Expressions

- C code

  ```c
  int	x, y;	
  y	=	(x-3)*(y+123456)
  ```

- Compilation

  ```pseudocode
  compile_expr(y = (x-3)*(y+123456))	
  			compile_expr((x-3)*(y+123456))	
  						compile_expr(x-3)	
  									compile_expr(x)	
  												LD(x,r1)	
  									compile_expr(3)	
  												CMOVE(3,r2)	
  									SUB(r1,r2,r1)	
  						compile_expr(y+123456)	
  									compile_expr(y)	
  												LD(y,r2)	
  									compile_expr(123456)	
  												LD(c1,r3)	
  									ADD(r2,r3,r2)
  						MUL(r1,r2,r1)	
  			ST(r1,y)
  ```

- Beta assembly code

  ```assembly
  x: 	LONG(0)	
  y: 	LONG(0)	
  c1: LONG(123456)	
  ...	
      LD(x, r1)	
      CMOVE(3, r2)	
      SUB(r1, r2,	r1)	
      LD(y, r2)	
      LD(c1, r3)	
      ADD(r2, r3,	r2)	
      MUL(r2,	r1,	r1)	
      ST(r1, y)	
  ```


### Compiling Statements

- Recursively

- C code

  ```c
  int	n	=	20;
  int	r;	
  r	=	1;	
  while	(n	>	0)	
    {	
  		r	=	r*n;	
  		n	=	n-1;	
  	}
  ```

- Beta assembly code

  ```assembly
  n:		LONG(20)	
  r:		LONG(0)	
  start:	
  			CMOVE(1,	r0)	
  			ST(r0,	r)	
  			LD(n,r1) 		// keep n in r1	
  			LD(r,r3) 		// keep r in r3	
  			BR(test)	
  loop:	
  			MUL(r1,	r3,	r3)	
  			SUBC(r1,	1,	r1)	
  test:	
  			CMPLT(r31,	r1,	r2)	
  			BT(r2,	loop)	
  done:	
  			ST(r1,n) 		// save final n	
  			ST(r3,r) 		// save final r
  ```

### Modern Compilers 

<img src="image.assets/Screen Shot 2021-09-12 at 11.54.13.png" alt="Screen Shot 2021-09-12 at 11.54.13" style="zoom:33%;" />

## *Thinking*

- This lecture introduces the assembly language UASM ("microassembler") designed for the course. Most of the content is introduced by 15-213 with specific details.
- What is new is the difference between compilation and interpretation and the specific compilation process.
  - Compilation is the translation of a high-level language into assembly language, which runs fast
  - An interpreter is a program that runs on a (relatively) complex machine, simulating the behavior of a simple machine, where a command that runs on a simple machine often corresponds to multiple commands on a complex machine
    - There can be multiple layers: Hardware (x86 Instrs) - Python Interpreter (Python) - SciPy (Applic Lang)



- 本讲介绍了为课程设计的汇编语言UASM（“microassembler”）。大部分内容是15-213介绍过了，有一些特定的细节。
- 新的内容是编译（compilation）和解释（interpretation）的区别，以及编译的具体过程。
  - 编译即将高级语言翻译为汇编语言，运行速度快
  - 解释器即一个在（相对）复杂机器上运行的程序，模拟一个简单机器上的行为，在简单机器上运行的一个命令往往在复杂机器上对应多个命令
    - 可以有多层：Hardware (x86 Instrs)——Python Interpreter (Python)——SciPy (Applic Lang)



# <u>L11: Procedures & Stacks</u>

### Procedures: A Software Abstraction

- Procedures: A Software Abstraction

### Implementing Procedures

- Option 1: Inlining
  - Compiler substitutes procedure call with body 
  - Problems: Code size, Recursion
- Option 2: Linking
  - Produce separate code for each procedure 
  - Caller evaluates input arguments, stores them and transfers control to the callee’s entry point 
  - Callee runs, stores result, transfers control to caller 

## Procedure Linkage

### Procedure Storage Needs

- Basic requirements for procedure calls:
  - Input arguments
  - Return address
  - Results
- Local storage:
  - Variables that compiler can't fit in registers
  - Space to save caller's register values for registers that we overwrite

- Each of these is specific to a particular activation of a procedure. We call them the procedure’s <u>activation record</u>

### Activation Records

```c
int fact(int n) {	
	if (n > 0) return n*fact(n - 1);	
	else return 1;	
}	
```

<img src="image.assets/Screen Shot 2021-09-12 at 12.20.03.png" alt="Screen Shot 2021-09-12 at 12.20.03" style="zoom:33%;" />

### Insight (ca. 1960): We Need a Stack!

- Need data structure to hold activation records
- Activation records are allocated and deallocated in last-in-first-out (LIFO) order 
- Stack: push, pop, access to top element

## Stack Implementation

- BETA CONVENTIONS: 
  - Dedicate a register for the Stack Pointer (SP = R29). 
  - Builds up (towards higher addresses) on PUSH 
  - SP points to first UNUSED location; locations with addresses lower than SP have been previously allocated. 
  - Discipline: can use stack at any time; but leave it as you found it
  - Reserve a large block of memory well away from our program and its data 
- BETA: We use only software conventions to implement our stack (many architectures dedicate hardware)

### Stack Management Macros

- `PUSH(RX)`: Push `Reg[x]` onto stack

  - ```pseudocode
    Reg[SP] ← Reg[SP] + 4;
    Mem[Reg[SP]-4] ← Reg[x];
    ```

  - ```assembly
    ADDC(R29, 4, R29)
    ST(RX, -4, R29)
    ```

- `POP(RX)`: Pop value on top of the stack into `Reg[x]` 

  - ```pseudocode
    Reg[x] ← Mem[Reg[SP]-4]	
    Reg[SP] ← Reg[SP] - 4;	
    ```

  - ```assembly
    LD(R29, -4, RX)	
    SUBC(R29, 4, R29)
    ```

- `ALLOCATE(k)`: Reserve k WORDS of stack

  - ```assembly
    ADDC(R29, 4*k, R29)
    ```

- `DEALLOCATE(k)`: Release k WORDS of stack

  - ```assembly
    SUBC(R29, 4*k, R29)
    ```

## Procedure Linkage: The Contract

## <img src="image.assets/Screen Shot 2021-09-12 at 12.52.33.png" alt="Screen Shot 2021-09-12 at 12.52.33" style="zoom:33%;" />

- The CALLER will:
  - Push args onto stack, in reverse order.
  - Branch to callee, putting return address into LP.
  - Remove args from stack on return.
- The CALLEE will:
  - Saving return address back to the caller (it’s in LP)
  - Saving BP of caller (pointer to caller’s activation record)
  - Allocating stack locations to hold local variables
  - Save any registers callee uses: “callee saves” convention 
  - Perform promised computation, leaving result in R0.
  - Branch to return address.
  - Leave stacked data intact, including stacked args.
  - Leave regs (except R0) unchanged.

### e.g. Factorial

```assembly
fact:	PUSH(LP) 	 			//	save linkages
			PUSH(BP)
      MOVE(SP,BP) 		//	new frame base
      PUSH(r1) 	 			//	preserve regs
      LD(BP,-12,r1) 	//	r1 ← n
      CMPLEC(r1,0,r0) //	if (n > 0)
      BT(r0,else)
      SUBC(r1,1,r1) 	//	r1 ← (n-1)
      PUSH(r1) 	 			//	push arg1
      BR(fact,LP) 		//	fact(n-1)
      DEALLOCATE(1) 	//	pop	arg1
      LD(BP,-12,r1) 	//	r1 ← n
      MUL(r1,r0,r0) 	//	r0 ← n*fact(n-1)
      BR(rtn)
else: CMOVE(1,r0) 		//	return 1
rtn: 	POP(r1) 	 			//	restore regs
      MOVE(BP,SP) 		//	Why?
      POP(BP) 	 			//	restore links
      POP(LP)
      JMP(LP) 	 			//	return
```



## *Thinking*

- This lecture introduces stack from how subroutines (aka procedure) are compiled.

  - Compilation method of subroutine

    - Inlining

      - Code may be too large, recursive calls unknown

    - Linking

      1. Need to store activation record (aka stack frame/activation frame)

        - Parameters, return address, return value, and local variables that register cannot hold and caller register that needs to be modified

      2. Using the stack for storage

        - The LIFO feature of the stack fits the pattern of subroutine calls.

  - Example

```c
int fact(int n) {	
	if (n > 0) return n*fact(n - 1);	
	else return 1;	
}	
```

```assembly
fact:	PUSH(LP) 	 			//	stack：保存 caller 的 Linkage Pointer（调用位置的下一行）
			PUSH(BP)				//	stack：保存 caller 的 Base Pointer
      MOVE(SP,BP) 		//	Base Pointer（R27）：设置为当前地址（Stack Pointer）
      PUSH(r1) 	 			//	stack：保存 R1（caller 中的值）作为本地变量备用
      LD(BP,-12,r1) 	//	R1：将第一个参数 n（位置为 BP-12，caller 中参数倒序保存）保存
      CMPLEC(r1,0,r0) //	R0：if （n <= 0）
      BT(r0,else)			//	brach else：Branch True（R0 == 1）
      SUBC(r1,1,r1) 	//	R1：递归参数计算（R1-1）
      PUSH(r1) 	 			//	stack：保存 R1 作为 callee 参数
      BR(fact,LP) 		//	branch fact：Brach Always，并将下一行保存入 Linkage Pointer
      DEALLOCATE(1) 	//	将 callee 参数移除
      LD(BP,-12,r1) 	//	R1：将第一个参数n（位置为 BP-12）保存至R1
      MUL(r1,r0,r0) 	//	R0：将 R1 和 R0（fact（n-1））的乘积保存入 R0 作为返回值
      BR(rtn)
else: CMOVE(1,r0) 		//	R0：将 1 保存入 R0 作为返回值
rtn: 	POP(r1) 	 			//	R1：恢复备用寄存器 R1（caller 中的值）
      MOVE(BP,SP) 		//	Stack Pointer：将 SP 移至 BP
      POP(BP) 	 			//	Base Pointer：恢复 caller 的 BP
      POP(LP)					//	Linkage Pointer：恢复 caller 的 LP
      JMP(LP) 	 			//	Stack Pointer：移至 caller 的 LP（调用位置的下一行）
```

- 本讲从子程序（procedure，subroutine）进行编译的方式引出stack。

  - 子程序（subroutine）的编译方式

    - 嵌入 inlining

      - 代码可能过大，递归调用次数未知

    - 链接 linking

      1. 需要存储激活记录 activation record（aka 栈帧 stack frame/activation frame）

        - 参数、返回地址、返回值，以及register容纳不了的本地变量和需要修改的caller register

      2. 使用stack进行存储

        - stack的LIFO特性恰好符合子程序调用的规律



# <u>L12: Stack Detective; Computability</u>

- Stack Detective
  -  ​	<img src="image.assets/Screen Shot 2021-09-13 at 15.54.35.png" alt="Screen Shot 2021-09-13 at 15.54.35" style="zoom:33%;" />

## University

- FSM Limitation
- Turing Machine
- Computability
- Interpretation

## *Thinking*

- 给出一个stack，回答一系列问题
- 可计算性

# <u>L13: Building the Beta</u>

## CPU Design Tradeoffs

- Maximum Performance
  - $\frac{\text { Time }}{\text { Program }}=\frac{\text { Instructions }}{\text { Program }} \cdot \frac{\text { Cycles }}{\text { Instruction }} \cdot \frac{\text { Time }}{\text { Cycle }}$
- Minimum Cost
- Best Performance/Price

## Beta

### Multi-Ported Register File

-  ​	<img src="image.assets/Screen Shot 2021-09-13 at 17.14.17.png" alt="Screen Shot 2021-09-13 at 17.14.17" style="zoom:33%;" />
   - 2 combinational READ ports
     - internal logic ensures `Reg[31]` reads as 0
   - 1 clocked WRITE port
-  Timing

### ALU Instructions

- FETCH (read) 32-bit instruction for the current cycle 
- DECODE instruction: ADD, SUB, XOR, etc 
- READ operands (Ra, Rb) from Register File 
- EXECUTE operation 
- WRITE-BACK result into Register File (Rc)

### Exception

- Exceptions
- Interrupts

### Beta: Our “Final Answer”

<img src="image.assets/Screen Shot 2021-09-13 at 17.11.11.png" alt="Screen Shot 2021-09-13 at 17.11.11" style="zoom: 50%;" />

- Control Signals
  - ALUFN: Arithmetic Logic Unit Function
  - ASEL: A Select
  - BSEL: B Select
  - MOE: Memory Output Enable
  - MWR: Memory Write (Enable)
  - PCSEL: Point Conter Select
  - RA2SEL: Register Address 2 Select
  - WASEL: (Register) Write Address Select
  - WDSEL: (Register) Write Data Select
  - WERF: Write Enable Register File
- Other Signals
  - C: SXT: Sign-extended Constant
  - JT: Jump Target
  - IRQ: Interrupt

### Control Logic

<img src="image.assets/Screen Shot 2021-09-13 at 17.17.31.png" alt="Screen Shot 2021-09-13 at 17.17.31" style="zoom: 33%;" />

## *Thinking*

- This lecture uses the hardware units (ROM, DRAM, Register, MUX, ALU) previously learned to build a complete CPU step by step.
- In particular, Incremental Featurism makes the overall construction have a clear logic and cause and effect.



- 本讲使用之前学习过的硬件单元（ROM，DRAM，Register，MUX，ALU）一步步来构建一个完整的CPU。内容很复杂，但通过之前的铺垫已经可以很好地理解其中的逻辑。
- 尤其是Incremental Featurism使得整体的构建有非常清晰的逻辑和因果。



# <u>L14: The Memory Hierarchy</u>

## Memory Technologies

|            | Capacity      | Latency | Cost/GB | U                  |
| ---------- | ------------- | ------- | ------- | ------------------ |
| Register   | 1000s of bits | 20 ps   | $$$$    | Processor Datapath |
| SRAM       | ~10 KB-10 MB  | 1-10 ns | ~$1000  | Memory Hierarchy   |
| DRAM       | ~10 GB        | 80 ns   | ~$10    | Memory Hierarchy   |
| Flash*     | ~100 GB       | 100 us  | ~$1     | I/O subsystem      |
| Hard disk* | ~1 TB         | 10 ms   | ~$0.10  | I/O subsystem      |

\*non-volatile (retains contents when powered off) 

### Static RAM (SRAM)

<img src="image.assets/Screen Shot 2021-09-13 at 18.59.28.png" alt="Screen Shot 2021-09-13 at 18.59.28" style="zoom: 25%;" />

- SRAM Cell
  -  ​	<img src="image.assets/Screen Shot 2021-09-13 at 18.59.52.png" alt="Screen Shot 2021-09-13 at 18.59.52" style="zoom:33%;" />
- Writes and Reads
- Summary
  - Array of k*b cells (k words, b cells per word) 
  - Cell is a bistable element + access transistors 
    - Analog circuit with carefully sized transistors to allow reads and writes 
  - Read: Precharge bitlines, activate wordline, sense
  - Write: Drive bitlines, activate wordline, overpower cells

### Dynamic Ram (DRAM)

- DRAM Cell
  -  ​	<img src="image.assets/Screen Shot 2021-09-13 at 19.01.38.png" alt="Screen Shot 2021-09-13 at 19.01.38" style="zoom:33%;" />
- Writes and Reads
- Summary
  - 1T DRAM cell: transistor + capacitor
  - Smaller than SRAM cell, but destructive reads and capacitors leak charge
  - DRAM arrays include circuitry to:
    - Write word again after every read (to avoid losing data) 
    - Refresh (read+write) every word periodically 

### DRAM vs. SRAM

- ~20x denser than SRAM 
- ~2-10x slower than SRAM 

### Non-Volatile Storage

- Flash Memory
  -  ​	<img src="image.assets/Screen Shot 2021-09-13 at 19.03.38.png" alt="Screen Shot 2021-09-13 at 19.03.38" style="zoom:33%;" />
  -  Very dense: Multiple bits/transistor, read and written in blocks 
  -  Slow (especially on writes), 10-100 us 
  -  Limited number of writes: charging/discharging the floating gate (writes) requires large voltages that damage transistor
- Hard Disk
  - Extremely slow (~10ms): Mechanically move head to position, wait for data to pass underneath head 

### Summary

- Different technologies have vastly different tradeoffs 
- Size is a fundamental limit, even setting cost aside: 
  - Small + low latency, high bandwidth, low energy, or 
  - Large + high-latency, low bandwidth, high energy

## The Memory Hierarchy

### Memory Hierarchy Interface

- Approach1: Exposed Hierarchy
  -    ​	<img src="image.assets/Screen Shot 2021-09-13 at 19.05.40.png" alt="Screen Shot 2021-09-13 at 19.05.40" style="zoom:33%;" />
  -    Tell programmers: “Use them cleverly” 
- Hide Hierarchy
  -    ​	<img src="image.assets/Screen Shot 2021-09-13 at 19.06.31.png" alt="Screen Shot 2021-09-13 at 19.06.31" style="zoom:33%;" />
  -    Programming model: Single memory, single address space

### The Locality Principle

- Access to address X at time t implies that access to address X+ΔX at time t+Δt becomes more probable as ΔX and Δt approach zero. 



## Caches

-  A small, interim storage component that transparently retains (caches) data from recently accessed locations 

### A Typical Memory Hierarchy

- Everything is a cache for something else…
  - ​	<img src="image.assets/Screen Shot 2021-09-13 at 19.08.04.png" alt="Screen Shot 2021-09-13 at 19.08.04" style="zoom:33%;" />


### Cache Metrics

- Recursively: 
  $$
  \text { AMAT }=\text { HitTime }_{L 1}+\text { MissRatio }_{L 1} \times\left(\text { HitTime }_{L 2}+\text { MissRatio }_{L 2} \times A M A T_{L 3}\right)=\ldots
  $$

## *Thinking*

- This talk introduces storage technologies at the hardware level: register, SRAM, DRAM, Flash, and Hard Disk, leading to the concepts of storage hierarchy and cache.
- Everything is a cache for something else...



- 本讲从硬件层面介绍了存储技术：register，SRAM，DRAM，Flash，Hard Disk，从而引出存储等级和缓存的概念：
- 一切都是其他事物的缓存。



# <u>L15: Hardware Caches</u>

## Direct-Mapped Caches

- Example: 8-location DM cache (W=3)
  -  ​	<img src="image.assets/Screen Shot 2021-09-14 at 11.22.37.png" alt="Screen Shot 2021-09-14 at 11.22.37" style="zoom: 25%;" />

### Block Size

- Take advantage of locality: increase block size
  - Another advantage: Reduces size of tag memory!
  - Potential disadvantage: Fewer blocks in the cache
- Example: 4-block, 16-word DM cache 
  -  ​	<img src="image.assets/Screen Shot 2021-09-14 at 10.29.44.png" alt="Screen Shot 2021-09-14 at 10.29.44" style="zoom: 25%;" />
- Tradeoffs
  -  ​	<img src="image.assets/Screen Shot 2021-09-14 at 10.31.18.png" alt="Screen Shot 2021-09-14 at 10.31.18" style="zoom: 25%;" />

### Direct-Mapped Cache Problem: Conflict Misses

### Fully-Associative Cache

- Opposite extreme: Any address can be in any location
  - No cache index!
  - Flexible (no conflict misses)
  - Expensive: Must compare tags of all entries in parallel to find matching one (can do this in hardware, this is called a CAM)
- Example
  -  ​	<img src="image.assets/Screen Shot 2021-09-14 at 11.23.21.png" alt="Screen Shot 2021-09-14 at 11.23.21" style="zoom: 25%;" />

## N-way Set-Associative Cache

- Compromise between direct-mapped and fully associative 
- Direct-mapped and fully-associative are just special cases of N-way set-associative 
- Example: 3-way 8-set cache 
  -  ​	<img src="image.assets/Screen Shot 2021-09-14 at 11.26.40.png" alt="Screen Shot 2021-09-14 at 11.26.40" style="zoom:25%;" />



### Associativity Tradeoffs

- More ways...
  - Reduce conflict misses
  - Increase hit time
    - ​	<img src="image.assets/Screen Shot 2021-09-14 at 11.28.53.png" alt="Screen Shot 2021-09-14 at 11.28.53" style="zoom:25%;" />

## Aux

### Replacement Policies

- Least Recently Used (LRU): Replace the block that was accessed furthest in the past 
  - Works well in practice
  - Need to keep ordered list of $\mathrm{N}$ items $\rightarrow \mathrm{N}$ ! orderings $\rightarrow O\left(\log _{2} \mathrm{~N} !\right)=\mathrm{O}\left(\mathrm{N} \log _{2} \mathrm{~N}\right)$ “LRU bits" $+$ complex logic
  - Caches often implement cheaper approximations of LRU

### Write Policy

- Write-through
- Write-behind
- Write-back: CPU writes are cached, but not written to main memory until we replace the block. Memory contents can be “stale”
  - Write-back with "Dirty" Bits: Add 1 bit per block to record whether block has been written to. Only write back dirty blocks. 

## Summary: Cache Tradeoffs 

- Larger cache size: Lower miss rate, higher hit time
- Larger block size: Trade off spatial for temporal locality, higher miss penalty
- More associativity (ways): Lower miss rate, higher hit time
- More intelligent replacement: Lower miss rate, higher cost
- Write policy: Lower bandwidth, more complexity
- How to navigate all these dimensions? Simulate different cache organizations on real programs

## *Thinking*

- Introduces the mechanism of caching, from the Directed Map to the N-way Associative Map, including the use of addresses, compared to the 15-213 more step-by-step derivation (more mathematical)



- 介绍了缓存的机制，从Directed Map到N-way Associative Map，包括地址的使用，相比15-213更加逐步推导（更数学）



# <u>Lab6. Procedures and Stacks</u>

## *Thinking*

- This lab exercises the Beta version of assembly language. The lecture software BSim is powerful. It can simulate the machine language, the data of registers, the data of Stack, the data of memory, and the state of Cache and display the state of the circuit diagram (schematic).
- It can be understood as building a simulated computer on a computer.
  - The simulator uses the most basic building blocks (in this case, MOSFETs, capacitors, and power supplies) to build a CPU that supports a series of opcodes step by step
  - On top of this CPU, an ISA (Instruction Set Architecture), a Disassembler, a memory simulator, and a cache simulator are added
  - This constitutes a complete modern computer (von Neumann architecture)
- Some features and caveats of the Beta version of assembly language.
  - PUSH and POP occupy two memory addresses
  - All commands are macros with opcodes supported by the hardware
  - Be careful with R0. Any procedure can change its value



- 这个lab练习了Beta版本的汇编语言，授课软件BSim非常强大，不仅可以模拟经过assembler反汇编后的机器语言、寄存器的数据、Stack的数据、内存的数据、Cache的状态，还可以显示电路图（schematic）的状态。
- 可以理解为在一个计算机上构建一台模拟计算机：
  - 该模拟机使用最基础的构件（此处为MOSFETs、电容和电源），一步步构建出支持一系列opcode的CPU
  - 在该CPU的基础上加入一套ISA（Instruction Set Architecture）、一个Disassembler，一个内存模拟器，一个缓存模拟器
  - 从而构成一个完整的现代计算机（冯·诺依曼结构，不包含）
- Beta版的汇编语言的一些特点和注意事项：
  - PUSH和POP实际上占用两个内存地址
  - 所有的命令实际上都是硬件支持的opcode（操作码）的macro（宏）
  - 小心R0的使用，任何procedure都可以改变它的值

# <u>Lab7. Building the Beta</u>

- TODO
  - [ ] optimise PCSELsel

## *Thinking*

- This lab completed the design of Beta, which itself is not difficult, mainly to deepen the understanding of CPU structure.
- Still very accomplished, the previous logic gates, full adder, and ALU are added to this design:
  - First, from MOSFETs to logic gates (logic operations, multiplexers, registers, etc.)
    - Use logic gates to complete the arithmetic-logic unit (ALU, the core is the full adder)
    - Use logic gates to complete the selection and transmission of signals
    - Use logic gates to complete the read and write of storage
      - Calculation of Program Counter by the full adder, multiplexer, and registers
      - Complete CPU Register File using memory unit (core) and multiplexer
      - Use the memory unit (core), multiplexer, and other logic gates to complete the control logic unit



- 这个lab完成了Beta的设计，本身并没有什么困难的地方，主要是加深对于CPU结构的理解。
- 还是很有成就感的，之前的逻辑门、全加器、ALU都加入了这次的设计：
  - 首先从MOSFETs开始构建逻辑门（逻辑运算，数据选择器，寄存器等）
    - 利用逻辑门完成算数逻辑运算单元（ALU，核心为全加器）
    - 利用逻辑门完成信号的选择和传输
    - 利用逻辑门完成存储的读写
      - 通过全加器、数据选择器和寄存器计算程序地址（Program Counter）
      - 利用存储单元（核心）和数据选择器完成CPU寄存器堆（Register File）
      - 利用存储单元（核心）、数据选择器和其他逻辑门完成控制逻辑单元（Control Logic）



# <u>Lab8. Caches</u>

## *Thinking*

- This lab familiarizes one with the relationship between cache associativity, block size, and data types and addresses.
- p2 introduces a circuit of Instruction Cache (2-way, block size: 2). The main contents include
  - A two-way memory
    - line: valid bit, tag, block
  - A FSM (seq logic) to determine if the data is ready
    - Three clock cycles to read from memory
  - A comb logic to determine if it is a hit or a miss
    - whether the tag is equal, valid, or reset
  - A comb logic selects instruction data
    - Select from path 1, path 2, or memory return value, and select word from block
  - A comb logic to determine the LRU (least recently used) and decide whether it misses or not, thus deciding the WE of the two-way store



- 这个lab通过练习熟悉了Cache的associativity，block size和数据类型及地址的关系。
- p2介绍了一个Instruction Cache的电路（2-way，block size：2），主要内容包括
  - 一个二路的存储
    - line：valid bit，tag，block
  - 一个FSM（seq logic）判断数据是否准备好
    - 从内存读取的话需要三个clock cycle
  - 一个comb logic判断是hit还是miss
    - 判断tag是否相等，是否valid，是否reset
  - 一个comb logic选择instruction data
    - 从路1或路2或内存返回值中选择，并从block中选择word
  - 一个comb logic来判断LRU（least recently used），并根据是否miss，从而决定二路存储的WE























